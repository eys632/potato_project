{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11aa2d80",
   "metadata": {},
   "source": [
    "### 데이터 비율 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f3cf39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 2.해충, Image Count: 1678\n",
      "Class: 3.충해, Image Count: 387\n",
      "Class: 0.정상, Image Count: 7567\n",
      "Class: 9.증강_(2), Image Count: 8980\n",
      "Class: 9.증강_(1), Image Count: 11670\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 데이터 디렉토리 경로 설정\n",
    "data_dir = '/home/a202192020/딥러닝/감자병해충/[T원천]01.감자'\n",
    "\n",
    "# 클래스별 데이터 개수를 저장할 딕셔너리\n",
    "class_counts = {}\n",
    "\n",
    "# 각 클래스 디렉토리에서 이미지 파일 개수 세기\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if os.path.isdir(class_path):  # 디렉토리인지 확인\n",
    "        image_count = len([file for file in os.listdir(class_path) if file.endswith('.jpg')])\n",
    "        class_counts[class_name] = image_count\n",
    "\n",
    "# 결과 출력\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class: {class_name}, Image Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbee2c7",
   "metadata": {},
   "source": [
    "json파일을 이용하여 전처리에 활용 (json을 이미지와 매칭), 학습에는 json 사용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e35c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 완료! 매칭된 이미지 파일 수: 36883\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# 경로 설정\n",
    "json_dir = '/home/a202192020/딥러닝/감자병해충/라벨링'  # JSON 파일의 최상위 폴더 경로\n",
    "image_dir = '/home/a202192020/딥러닝/감자병해충/[T원천]01.감자'  # 이미지 파일 경로\n",
    "output_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data'  # 전처리된 데이터를 저장할 폴더\n",
    "\n",
    "# 출력 폴더 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 매칭된 파일 개수\n",
    "matched_count = 0\n",
    "\n",
    "# JSON 파일 처리\n",
    "for root, _, files in os.walk(json_dir):\n",
    "    for json_file in files:\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(root, json_file)\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    # JSON 내용 로드\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # 이미지 파일 이름 추출\n",
    "                    image_filename = data[\"description\"][\"image\"]\n",
    "                    \n",
    "                    # 이미지 파일 실제 경로 찾기\n",
    "                    image_path = None\n",
    "                    for dirpath, _, image_files in os.walk(image_dir):\n",
    "                        if image_filename in image_files:\n",
    "                            image_path = os.path.join(dirpath, image_filename)\n",
    "                            break\n",
    "                    \n",
    "                    # 클래스 이름 추출 (기본은 JSON 파일의 상위 디렉토리 이름)\n",
    "                    class_name = os.path.basename(os.path.dirname(json_path))\n",
    "                    \n",
    "                    # 정상 데이터를 처리\n",
    "                    if \"정상\" in class_name:\n",
    "                        class_name = \"정상\"\n",
    "\n",
    "                    # 증강 데이터는 추가적으로 어떤 데이터를 증강했는지 확인\n",
    "                    elif \"증강\" in class_name:\n",
    "                        parent_dir = os.path.basename(os.path.dirname(root))\n",
    "                        if \"충해\" in parent_dir:\n",
    "                            class_name = \"충해_증강\"\n",
    "                        elif \"해충\" in parent_dir:\n",
    "                            class_name = \"해충_증강\"\n",
    "                        else:\n",
    "                            class_name = \"증강_기타\"\n",
    "\n",
    "                    # 이미지 파일 복사\n",
    "                    if image_path and os.path.exists(image_path):\n",
    "                        # 클래스별 폴더 생성\n",
    "                        label_dir = os.path.join(output_dir, class_name)\n",
    "                        os.makedirs(label_dir, exist_ok=True)\n",
    "                        \n",
    "                        # 이미지 복사\n",
    "                        shutil.copy(image_path, label_dir)\n",
    "                        matched_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {json_path}: {e}\")\n",
    "\n",
    "print(f\"전처리 완료! 매칭된 이미지 파일 수: {matched_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3fb982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: [T라벨링]01.감자_3.충해, Image Count: 388\n",
      "Class: 증강_기타, Image Count: 21950\n",
      "Class: 정상, Image Count: 12738\n",
      "Class: [T라벨링]01.감자_2.해충, Image Count: 1807\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Processed_Data 경로 설정\n",
    "processed_data_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data'\n",
    "\n",
    "# 클래스별 데이터 개수를 저장할 딕셔너리\n",
    "class_counts = {}\n",
    "\n",
    "# 각 클래스 디렉토리에서 이미지 파일 개수 세기\n",
    "for class_name in os.listdir(processed_data_dir):\n",
    "    class_path = os.path.join(processed_data_dir, class_name)\n",
    "    if os.path.isdir(class_path):  # 폴더인지 확인\n",
    "        image_count = len([file for file in os.listdir(class_path) if file.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        class_counts[class_name] = image_count\n",
    "\n",
    "# 결과 출력\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class: {class_name}, Image Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396cf80",
   "metadata": {},
   "source": [
    "데이터 불균형을 해결하기 위해 증강_기타 데이터 판별(예시로 3개만 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed63665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[이미지 파일]: V006_78_3_19_01_03_12_3_6813q_20200918_8_a0003.jpg\n",
      "[JSON 파일]: /home/a202192020/딥러닝/감자병해충/라벨링/Training/[T라벨링]01.감자_9.증강/V006_78_3_19_01_03_12_3_6813q_20200918_8_a0003.jpg.json\n",
      "[원본 이미지 파일]: V006_78_3_19_01_03_12_3_6813q_20200918_8.jpg\n",
      "[원래 클래스]: [T라벨링]01.감자_3.충해\n",
      "\n",
      "[이미지 파일]: V006_78_3_19_01_03_12_2_1097q_20201015_90_a0005.jpg\n",
      "[JSON 파일]: /home/a202192020/딥러닝/감자병해충/라벨링/Training/[T라벨링]01.감자_9.증강/V006_78_3_19_01_03_12_2_1097q_20201015_90_a0005.jpg.json\n",
      "[원본 이미지 파일]: V006_78_3_19_01_03_12_2_1097q_20201015_90.jpg\n",
      "[원래 클래스]: [T라벨링]01.감자_3.충해\n",
      "\n",
      "[이미지 파일]: V006_78_2_19_01_07_32_0_0719y_20201022_5_a0000.jpeg\n",
      "[JSON 파일]: /home/a202192020/딥러닝/감자병해충/라벨링/Training/[T라벨링]01.감자_9.증강/V006_78_2_19_01_07_32_0_0719y_20201022_5_a0000.jpeg.json\n",
      "[원본 이미지 파일]: V006_78_2_19_01_07_32_0_0719y_20201022_5.jpeg\n",
      "[원래 클래스]: [T라벨링]01.감자_2.해충\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# 경로 설정\n",
    "json_dir = '/home/a202192020/딥러닝/감자병해충/라벨링'  # JSON 파일 경로\n",
    "image_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data/증강_기타'  # 증강_기타 이미지 경로\n",
    "\n",
    "# 증강_기타 이미지에서 랜덤으로 샘플 추출\n",
    "random_sample = random.sample(\n",
    "    [file for file in os.listdir(image_dir) if file.lower().endswith(('.jpg', '.jpeg', '.png'))], 3\n",
    ")\n",
    "\n",
    "# 랜덤으로 선택된 이미지 처리\n",
    "for image_file in random_sample:\n",
    "    print(f\"\\n[이미지 파일]: {image_file}\")\n",
    "    json_matched = False\n",
    "    for root, _, files in os.walk(json_dir):\n",
    "        for json_file in files:\n",
    "            if json_file.endswith('.json'):\n",
    "                json_path = os.path.join(root, json_file)\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        data = json.load(f)\n",
    "                        if data[\"description\"][\"image\"] == image_file:\n",
    "                            print(f\"[JSON 파일]: {json_path}\")\n",
    "                            original_image = data[\"description\"].get(\"original\", None)\n",
    "                            if original_image:\n",
    "                                print(f\"[원본 이미지 파일]: {original_image}\")\n",
    "                                # 원본 이미지의 클래스 찾기\n",
    "                                for orig_root, _, orig_files in os.walk(json_dir):\n",
    "                                    for orig_json in orig_files:\n",
    "                                        if orig_json.endswith('.json'):\n",
    "                                            orig_json_path = os.path.join(orig_root, orig_json)\n",
    "                                            with open(orig_json_path, 'r', encoding='utf-8') as orig_f:\n",
    "                                                orig_data = json.load(orig_f)\n",
    "                                                if orig_data[\"description\"][\"image\"] == original_image:\n",
    "                                                    found_class = os.path.basename(os.path.dirname(orig_json_path))\n",
    "                                                    print(f\"[원래 클래스]: {found_class}\")\n",
    "                                                    json_matched = True\n",
    "                                                    break\n",
    "                                    if json_matched:\n",
    "                                        break\n",
    "                        if json_matched:\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {json_path}: {e}\")\n",
    "        if json_matched:\n",
    "            break\n",
    "    if not json_matched:\n",
    "        print(\"[JSON 파일]: 매칭되는 JSON 파일을 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9405af7",
   "metadata": {},
   "source": [
    "전부 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf7da75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "증강_기타 데이터의 원래 클래스 분포:\n",
      "[T라벨링]01.감자_2.해충: 18070\n",
      "[T라벨링]01.감자_3.충해: 3880\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# 경로 설정\n",
    "json_dir = '/home/a202192020/딥러닝/감자병해충/라벨링'  # JSON 파일 경로\n",
    "image_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data/증강_기타'  # 증강_기타 이미지 경로\n",
    "output_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data'  # 결과 저장 경로\n",
    "\n",
    "# 결과 저장 딕셔너리\n",
    "class_origin_counts = {}\n",
    "\n",
    "# 증강_기타 내 이미지 파일 처리\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        for root, _, files in os.walk(json_dir):\n",
    "            for json_file in files:\n",
    "                if json_file.endswith('.json'):\n",
    "                    json_path = os.path.join(root, json_file)\n",
    "                    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                        try:\n",
    "                            data = json.load(f)\n",
    "                            if data[\"description\"][\"image\"] == image_file:\n",
    "                                # \"original\" 필드에서 원본 파일 이름 확인\n",
    "                                original_image = data[\"description\"].get(\"original\", None)\n",
    "                                if original_image:\n",
    "                                    # 원본 이미지의 클래스 찾기\n",
    "                                    found_class = None\n",
    "                                    for orig_root, _, orig_files in os.walk(json_dir):\n",
    "                                        for orig_json in orig_files:\n",
    "                                            if orig_json.endswith('.json'):\n",
    "                                                orig_json_path = os.path.join(orig_root, orig_json)\n",
    "                                                with open(orig_json_path, 'r', encoding='utf-8') as orig_f:\n",
    "                                                    orig_data = json.load(orig_f)\n",
    "                                                    if orig_data[\"description\"][\"image\"] == original_image:\n",
    "                                                        # 원본 클래스 추출\n",
    "                                                        found_class = os.path.basename(os.path.dirname(orig_json_path))\n",
    "                                                        break\n",
    "                                        if found_class:\n",
    "                                            break\n",
    "                                    \n",
    "                                    # 원본 클래스에 따라 분류\n",
    "                                    if found_class:\n",
    "                                        class_origin_counts[found_class] = class_origin_counts.get(found_class, 0) + 1\n",
    "                                        target_dir = os.path.join(output_dir, found_class)\n",
    "                                        os.makedirs(target_dir, exist_ok=True)\n",
    "                                        shutil.copy(os.path.join(image_dir, image_file), target_dir)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {json_path}: {e}\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"증강_기타 데이터의 원래 클래스 분포:\")\n",
    "for class_name, count in class_origin_counts.items():\n",
    "    print(f\"{class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ad83265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: [T라벨링]01.감자_3.충해, Image Count: 4268\n",
      "Class: 증강_기타, Image Count: 21950\n",
      "Class: 정상, Image Count: 12738\n",
      "Class: [T라벨링]01.감자_2.해충, Image Count: 19877\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Processed_Data 경로 설정\n",
    "processed_data_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data'\n",
    "\n",
    "# 클래스별 데이터 개수를 저장할 딕셔너리\n",
    "class_counts = {}\n",
    "\n",
    "# 각 클래스 디렉토리에서 이미지 파일 개수 세기\n",
    "for class_name in os.listdir(processed_data_dir):\n",
    "    class_path = os.path.join(processed_data_dir, class_name)\n",
    "    if os.path.isdir(class_path):  # 폴더인지 확인\n",
    "        image_count = len([file for file in os.listdir(class_path) if file.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        class_counts[class_name] = image_count\n",
    "\n",
    "# 결과 출력\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class: {class_name}, Image Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fdf5ff",
   "metadata": {},
   "source": [
    "증강_기타 데이터는 클래스가 명확하지 않기 때문에 활용하지 않음. 남은 데이터들을 정상/비정상으로 분류함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf4b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: [T라벨링]01.감자_3.충해, Image Count: 4268\n",
      "Class: 증강_기타, Image Count: 21950\n",
      "Class: 정상, Image Count: 12738\n",
      "Class: [T라벨링]01.감자_2.해충, Image Count: 19877\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Processed_Data 경로 설정\n",
    "processed_data_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data'\n",
    "\n",
    "# 클래스별 데이터 개수를 저장할 딕셔너리\n",
    "class_counts = {}\n",
    "\n",
    "# 각 클래스 디렉토리에서 이미지 파일 개수 세기\n",
    "for class_name in os.listdir(processed_data_dir):\n",
    "    class_path = os.path.join(processed_data_dir, class_name)\n",
    "    if os.path.isdir(class_path):  # 폴더인지 확인\n",
    "        image_count = len([file for file in os.listdir(class_path) if file.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        class_counts[class_name] = image_count\n",
    "\n",
    "# 결과 출력\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class: {class_name}, Image Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61db7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving corrupted image: /home/a202192020/딥러닝/감자병해충/Processed_Data/[T라벨링]01.감자_3.충해/V006_78_3_19_01_03_12_3_6813q_20201015_93.jpg\n",
      "손상된 파일 및 비이미지 파일 이동 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# 데이터 디렉토리 설정\n",
    "data_dir = '/home/a202192020/딥러닝/감자병해충/Processed_Data'\n",
    "problematic_dir = '/home/a202192020/딥러닝/감자병해충/Problematic_Files'\n",
    "\n",
    "# 손상된 파일 이동 디렉토리 생성\n",
    "os.makedirs(problematic_dir, exist_ok=True)\n",
    "\n",
    "# 손상된 파일 및 비이미지 파일 찾기 및 이동\n",
    "def move_problematic_files(directory, output_directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img.verify()  # 이미지가 열리는지 검증\n",
    "                except (IOError, SyntaxError):\n",
    "                    print(f\"Moving corrupted image: {file_path}\")\n",
    "                    os.rename(file_path, os.path.join(output_directory, os.path.basename(file_path)))\n",
    "            else:\n",
    "                print(f\"Moving non-image file: {file_path}\")\n",
    "                os.rename(file_path, os.path.join(output_directory, os.path.basename(file_path)))\n",
    "\n",
    "move_problematic_files(data_dir, problematic_dir)\n",
    "print(\"손상된 파일 및 비이미지 파일 이동 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96958159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 05:13:18.096557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-30 05:13:18.209988: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-30 05:13:18.818693: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-30 05:13:18.818768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-30 05:13:18.818776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-12-30 05:13:19.751954: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-30 05:13:20.704284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38357 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:3b:00.0, compute capability: 8.0\n",
      "2024-12-30 05:13:20.706283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37940 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:5e:00.0, compute capability: 8.0\n",
      "2024-12-30 05:13:20.708142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37286 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:86:00.0, compute capability: 8.0\n",
      "2024-12-30 05:13:20.710349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37822 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "사용 가능한 GPU 수: 4\n",
      "Found 11948 images belonging to 3 classes.\n",
      "Found 2985 images belonging to 3 classes.\n",
      "클래스 가중치: {0: 1.272895467160037, 1: 5.740166865315852, 2: 24.88888888888889}\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 05:13:58.257441: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017TensorDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 05:14:35.565022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8800\n",
      "2024-12-30 05:14:35.590871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8800\n",
      "2024-12-30 05:14:35.622478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8800\n",
      "2024-12-30 05:14:35.655005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8800\n",
      "2024-12-30 05:14:35.692345: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-12-30 05:14:36.234559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-30 05:14:37.912598: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x37ebdce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-30 05:14:37.912691: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-12-30 05:14:37.912716: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-12-30 05:14:37.912737: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-12-30 05:14:37.912756: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-12-30 05:14:37.929895: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-30 05:14:38.027248: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-12-30 05:14:38.087404: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 2.5355 - accuracy: 0.6625 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 05:26:39.504946: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020TensorDataset:41\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 927s 39s/step - loss: 2.5355 - accuracy: 0.6625 - val_loss: 0.3692 - val_accuracy: 0.8791\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 891s 37s/step - loss: 1.5665 - accuracy: 0.8487 - val_loss: 0.5255 - val_accuracy: 0.7467\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 888s 37s/step - loss: 1.2470 - accuracy: 0.8785 - val_loss: 0.2816 - val_accuracy: 0.8640\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 896s 38s/step - loss: 1.0903 - accuracy: 0.8955 - val_loss: 0.4562 - val_accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 893s 37s/step - loss: 0.9417 - accuracy: 0.9151 - val_loss: 0.2973 - val_accuracy: 0.8700\n",
      "모델이 저장되었습니다: /home/a202192020/딥러닝/감자병해충/model_multiclass_classification_weighted_parallel.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 다중 GPU 전략 설정\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"사용 가능한 GPU 수: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# 데이터 경로 설정\n",
    "data_dir = '/home/a202192020/딥러닝/감자병해충/[T원천]01.감자'\n",
    "\n",
    "# 데이터 증강 및 전처리\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # 데이터 정규화\n",
    "    validation_split=0.2  # 80% 학습, 20% 검증\n",
    ")\n",
    "\n",
    "# 학습 데이터 로더\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(64, 64),  # 작은 이미지 크기로 속도 향상\n",
    "    batch_size=512,        # 병렬 처리를 위해 배치 크기 증가\n",
    "    class_mode='categorical',  # 다중 클래스 분류\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    classes=['0.정상', '2.해충', '3.충해']  # 사용할 클래스 명시\n",
    ")\n",
    "\n",
    "# 검증 데이터 로더\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=512,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    classes=['0.정상', '2.해충', '3.충해']  # 사용할 클래스 명시\n",
    ")\n",
    "\n",
    "# 클래스별 데이터 개수\n",
    "class_counts = {\n",
    "    '0.정상': 7567,\n",
    "    '2.해충': 1678,\n",
    "    '3.충해': 387\n",
    "}\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "total_samples = sum(class_counts.values())\n",
    "class_weights = {\n",
    "    0: total_samples / class_counts['0.정상'],  # 정상\n",
    "    1: total_samples / class_counts['2.해충'],  # 해충\n",
    "    2: total_samples / class_counts['3.충해']   # 충해\n",
    "}\n",
    "\n",
    "print(\"클래스 가중치:\", class_weights)\n",
    "\n",
    "# 모델 정의\n",
    "with strategy.scope():  # 다중 GPU 병렬 처리\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # 과적합 방지\n",
    "        Dense(3, activation='softmax')  # 클래스가 3개이므로 softmax 사용\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',  # 다중 클래스 손실 함수\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=5,  # 빠르게 끝내기 위해 에폭 최소화\n",
    "    class_weight=class_weights  # 클래스 가중치 적용\n",
    ")\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path = '/home/a202192020/딥러닝/감자병해충/model_multiclass_classification_weighted_parallel.h5'\n",
    "model.save(model_save_path)\n",
    "print(f\"모델이 저장되었습니다: {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
